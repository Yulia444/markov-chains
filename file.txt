События прошедшего столетия, такие как мировые войны, унесшие беспрецедентное количество жизней, экспонентное развитие и применение технологий, неузнаваемо трансформировавшие ландшафт политической, экономической, социокультурной и повседневной сферы, полностью изменили научную и общественную парадигму, создав новые машины, которые уже сегодня наравне с людьми выполняют вычислительные и логические задачи в различных отраслях. Активное развитие и внедрение технологии ИИ требует обсуждения самых различных аспектов создания искусственного интеллекта и возможных последствий его применения для человека и общества. Проблема в том, что вопросы и ответы на них зависят от того, что вкладывается в понятие «ИИ», а единства в этой теме не наблюдается.

При использовании ИИ появляется множество этических вопросов, которые представляются запутанными головоломками. Машинное обучение позволяет использовать прецеденты для того, чтобы машина впоследствии решала практические задачи на основе выработанных алгоритмов. Но что, если сама задача будет неэтичной? Например, сократить государственные расходы любой ценой? Вероятно, в этом случае решение алгоритмов затронуло бы интересы наиболее уязвимых слоев населения.

Создание полностью автономного оружия — это не только кошмарный сценарий фантастического фильма, но и насущная этическая проблема, требующая однозначного решения.

Ситуация осложняется тем, что этику непросто формализовать — эта философская дисциплина не универсальна. В различных государствах существуют собственные этические установки. Этика существенно меняется в ходе исторического развития и политических преобразований. В одних государствах давно существует мораторий на смертную казнь и ограничен срок максимального заключения, а в целых регионах мира можно получить смертный приговор или пожизненное заключение за считающийся в другой стране невинным проступок. Это еще больше осложняет вопрос этических установок ИИ, потому что положительные и отрицательные этические оценки не могут быть общепринятыми.

Работа над созданием свода этических норм не может быть осуществлена исключительно усилиями разработчиков ИИ. Их принятие и дальнейшее соблюдение должны регулироваться государством и международными организациями. Закон и этические нормы взаимосвязаны: моральные и нравственные принципы — это система координат для законодателей, а лучшим мотиватором для исполнения закона является его соответствие нравственным нормам. И с этой точки зрения целесообразно привлечение общественных организаций к активному участию в этическом регулировании ИИ, поскольку это обеспечит повышение уровня общественного интереса к этой важной сфере и вовлечения общественности в решение этой проблемы.

Мир действительно меняется, технологии, связанные с ИИ, уже занимают свое место в обществе. Наступает конец антропоцентризму. Человек перестает быть мерой всех вещей, и единственный для него способ избежать вырождения и морального устаревания — принять на себя всю полноту ответственности и осознанно регулировать создание и использование искусственного интеллекта.

Само название статьи может вселить чувство неопределенности при кажущейся на первый взгляд конкретике заданной темы: ведь ученые, разработчики и обыватели под искусственным интеллектом (ИИ) зачастую понимают совершенно разное. Поэтому мы начнем с определений и понятийного аппарата, чтобы избежать спекулятивных и непрофессиональных суждений. Вспомним, что этика — это философская дисциплина, изучающая мораль и нравственность. Это одна из древнейших областей философского знания, изначально целью которой являлось установление правил поведения людей для сплочения общества, его защиты от распада и проявления индивидуальной агрессии. На первый взгляд, это абсолютно «человеческая» проблематика, и в ней нет места машине и технологиям. Однако события прошедшего столетия, такие как мировые войны, унесшие беспрецедентное количество жизней, экспонентное развитие и применение технологий, до неузнаваемости трансформировавшие ландшафт политической, экономической, социокультурной и повседневной сферы, полностью изменили научную и общественную парадигму, создав новые машины, которые уже сегодня наравне с людьми выполняют вычислительные и логические задачи в различных отраслях. Активное развитие и внедрение технологии ИИ требует обсуждения самых различных аспектов создания искусственного интеллекта и возможных последствий его применения для человека и общества. Проблема в том, что вопросы и ответы на них зависят от того, что вкладывается в понятие «ИИ», а единства в этой теме не наблюдается.

Существует философия ИИ, которая изучает проблемы создания ИИ (дает ответ на вопрос, что это такое?) и изменений в широком смысле человеческого бытия в связи с его использованием. Цель этики ИИ, с одной стороны, — установление правил поведения для специалистов, разрабатывающих и конструирующих объекты с искусственным интеллектом, и для пользователей этих объектов (как к ним относиться, что допустимо, а что нет). С другой стороны, этика ИИ изучает требования к моральному поведению самих объектов с искусственным интеллектом, поскольку в этой части необходимо установить нормы и ограничения для ИИ в отношении общества и человека.

Следует обратить внимание на то, что ИИ — понятие достаточно абстрактное. Ученые не могут прийти к единому мнению относительно того, что представляет собой человеческий интеллект, и переносят частные или узкоспециализированные определения (с различными оговорками) на продукты технологий. Отметим, что практики часто условно разделяют ИИ на «сильный» и «слабый», чтобы провести черту между текущими техническими возможностями и мечтами (или кошмарами, в зависимости от точки зрения). «Слабый» (narrow, узконаправленный) ИИ — это активно развивающиеся и уже реализованные на основе технологий машинного обучения решения для выполнения сложных прикладных задач, от распознавания изображений до управления транспортными средствами. А «сильный» (true, настоящий) ИИ — это нечто, созданное искусственно, но имеющее сознание и способное мыслить, умеющее учиться. «Сильного» ИИ пока не существует, под вопросом и сама возможность его создания.

При использовании (возможном или фактическом) обоих видов ИИ возникает множество этических вопросов, которые представляются запутанными головоломками. Машинное обучение позволяет использовать прецеденты для того, чтобы машина впоследствии решала практические задачи на основе выработанных алгоритмов. Но что, если сама задача будет неэтичной? Например, сократить государственные расходы любой ценой. Вероятно, в этом случае решение алгоритмов затронуло бы интересы наиболее уязвимых слоев населения.

Создание полностью автономного оружия — это не только кошмарный сценарий фантастического фильма, но и насущная этическая проблема, требующая однозначного решения. Хотя часть исследователей считает,  что автономные роботы являются даже более гуманными, чем человек, поскольку принимают решения эффективнее, открытым остается вопрос, как может быть установлена и определена сама моральная основа для подобной деятельности. Да и кто должен нести ответственность в случае возможных трагических ошибок: государство, командир подразделения, разработчик программного обеспечения, или еще кто-то? Европейская позиция однозначна: в 2018 году Группа по этике в науке и новых технологиях в составе Европейской комиссии многие сферы: система оценки рисков возможных преступлений, созданная в США для полиции Чикаго, оказалась бесполезной, потому что изначально оказалась предвзятой по отношению к чернокожему населению — и в реальности криминогенная обстановка не стала менее острой, а вместо этого уровень социальной напряженности только повысился.

Продолжая тему проблемы недостаточной объективности алгоритмов, добавим, что они могут быть созданы таким образом, чтобы приносить совершенно практические дивиденды нечистым на руку создателям. Например, в области медицины можно настроить систему диагностики таким образом, чтобы максимизировать программы лечения и профилактики без оглядки на действительно требуемые показания. 
Более того, сам человек, становясь пользователем алгоритмов, в некоторой степени может загнать себя самого в ловушку, незаметно обеднив и сузив собственную картину мира и кругозор. Например, при использовании системы рекомендаций подходящих фильмов, созданной на основе машинного обучения, со временем зритель становится потребителем только тех фильмов, которые ему должны понравиться, перестает получать рекомендации к просмотру кинокартин непривычных жанров или снятых новыми режиссерами, и таким образом он теряет доступ к большей части кинематографического рынка. Несмотря на то, что стандартной практикой в рекомендательных системах является добавление в выдачу определенного количества случайных объектов, чтобы уменьшить описанную проблему, выбор человека почти всегда склоняется к знакомому и привычному.
Этот частный пример заставляет задуматься о гораздо более серьезных рисках применения технологий ИИ, лежащих в этической сфере. Согласно Гегелю, человек отличается от животных тем, что одной из целей его жизни является достижение признания. Эта потребность привела не только к консюмеризму, но и к развитию социальных сетей, погружение в которые для многих стало важной частью ежедневного распорядка дня. Первые серии «Черного зеркала» казались антиутопией, а всего лишь несколько лет спустя, сбор социальными сетями огромного количества личной информации, основанный на технологиях ИИ для маркетинга и рекламы, стал буквально вездесущим. Все это сокращает пространство свободы выбора для человека, сужает область его свободы, или, иными словами, автономии.
Однако, рисуя алармистские картины, нельзя забывать о том, что машины начали выполнять определенные задачи уже достаточно давно, столетия назад. И в течение всего этого времени люди беспокоились и спорили, как относиться к изменениям, зачастую выдвигая апокалиптические прогнозы. Сейчас перед человечеством снова стоит непростая и насущная задача — определить возможности и границы применения ИИ, потому что даже если эти технологии не поменяют качественно все сферы жизни, в любом случае они окажут на них огромное влияние. И это только начало: успехи, достигнутые на первоначальном этапе создания ИИ, вызывают оптимизм, и согласно прогнозу компании Tractica, исследующей перспективы развития технологий, инвестиции в развитие ИИ к 2025 году превысят 300 млрд долларов США во всем мире.
В связи с этим глобальные структуры не могут не реагировать на «гуманитарные» вызовы: правительство Великобритании разрабатывает этические основы использования данных и ИИ, а такие продвинутые компании, как Google, системно обсуждают моральные и этические нормы высоких технологий.

Ситуация осложняется тем, что этику непросто формализовать — эта философская дисциплина не универсальна. В различных государствах существуют собственные этические установки. Этика существенно меняется в ходе исторического развития и политических преобразований. В одних государствах давно существует мораторий на смертную казнь и ограничен срок максимального заключения, а в целых регионах мира можно получить смертный приговор или пожизненное заключение за считающийся в другой стране невинным проступок. Это еще больше осложняет вопрос этических установок ИИ, потому что положительные и отрицательные этические оценки не могут быть общепринятыми.

Обращаясь к практической сфере применения ИИ, приведем еще один конкретный пример. В мире нет единой позиции относительно того, насколько и в каких объемах допустимо контролировать перемещение граждан и использовать технологию распознавания лиц при помощи камер наружного наблюдения, подключенных к ИИ. Что касается Китая, то в этой стране такой вопрос не стоит: на сегодняшний день в КНР более 170 миллионов камер видеонаблюдения, и в ближайшие несколько лет данная цифра увеличится более чем в 2 раза. С помощью системы распознавания лиц во всей стране с 2020 года будет развернута система «рейтинга общественной надежности», которая сейчас работает пока в нескольких районах Китая. Гражданин имеет возможность поддержания и даже повышения рейтинга, аккуратно следуя правилам дорожного движения, выступая в качестве донора, ведя активную общественную работу. А те, кто курит в неположенных местах, покупает много алкогольных напитков, кстати, точного списка благих и порочных дел не опубликовано, могут легко оказаться на задворках жизни: по сообщениям СМИ, еще в 2018 году на основании результатов, полученных и обработанной ИИ, около 17,5 миллионов неблагонадежных граждан были наказаны запретом пользоваться авиатранспортом, а более 5 миллионам граждан запрещена покупка билетов на скоростные поезда. Совсем пугающими выглядят некоторые детали работы системы, просачивающиеся в СМИ, согласно которым электронное общение с неблагонадежным участником системы «рейтинга общественной надежности» негативно влияет на рейтинг другого участника.

Реализация описанного выше проекта, возможно, вписывающегося в рамки китайской культурной парадигмы, которая акцентирует внимание на гармонии и порядке, как высших ценностях, явилось бы кошмарной перспективой для западного общества. Даже американские технологические гиганты, разрабатывающие ИИ, такие как упомянутый выше Google, а также Apple, Facebook, Microsoft, принимают на работу специалистов по этике и философов для разработки этических стандартов и выстраивания внутренних процессов в соответствии с ними. В рамках совместной деятельности технические специалисты, ученые-гуманитарии и менеджеры должны создать систему бизнеса, учитывающую этические нормы.
Однако работа над созданием свода этических норм не может быть осуществлена исключительно усилиями разработчиков ИИ. Их принятие и дальнейшее соблюдение должны регулироваться государством и международными организациями. Закон и этические нормы взаимосвязаны: моральные и нравственные принципы — это система координат для законодателей, а лучшим мотиватором для исполнения закона является его соответствие нравственным нормам. И с этой точки зрения целесообразно привлечение общественных организаций к активному участию в этическом регулировании ИИ, поскольку это обеспечит повышение уровня общественного интереса к этой важной сфере и вовлечения общественности в решение этой проблемы.

Подобно тому, как в России в 2018 году была создана Ассоциация больших данных, целью которой является создание условий для развития технологий и продуктов в сфере больших данных и формирование кодекса этики использования больших данных для защиты интересов пользователей, имеет смысл задуматься о создании государственного или общественного института, который сможет стать регулятором и гарантом ответственного развития и внедрения ИИ, и обеспечит защиту как государственных, так и общественных интересов.

Мир действительно меняется, технологии, связанные с ИИ, уже занимают свое место в обществе. Наступает конец антропоцентризму. Человек перестает быть мерой всех вещей, и единственный для него способ избежать вырождения и морального устаревания — принять на себя всю полноту ответственности и осознанно регулировать создание и использование искусственного интеллекта.
Алгоритмы ИИ обучаются с помощью набора данных, которые используются для информирования или построения алгоритма. Если ваш алгоритм идентифицирует кита как лошадь, очевидно, что вам нужно предоставить больше данных о китах (и лошадях). Аналогично, если ваш алгоритм идентифицирует животных как людей, вам необходимо предоставить больше данных о людях. Если ваш алгоритм принимает неточные или неэтические решения, это может означать, что было недостаточно данных для обучения модели или что обучение с подкреплением не соответствовало желаемому результату.

Конечно, также возможно, что люди невольно ввели неэтические значения в систему с помощью предвзятого выбора данных или неправильно назначенных значений подкрепления. В целом, мы должны убедиться, что входные данные и материалы, которые мы предоставляем, рисуют полную и правильную картину для алгоритмов.
Чтобы отдельные лица могли обеспечивать соблюдение политики, технология должна позволять людям вносить изменения. Люди должны иметь возможность выбирать и корректировать учебные данные, контролировать источники данных и выбирать, как данным преобразовываться. Аналогичным образом, ИИ-технологии должны поддерживать надежное управление, включая доступ к данным и способность направлять алгоритмы, когда они неверны или работают за пределами этически определенных границ.

Невозможно предвидеть все потенциальные сценарии с ИИ, но важно рассмотреть возможности и установить средства контроля для положительного и отрицательного усиления. Например, введение новых, даже конкурирующих целей, может поощрить этические решения и идентифицировать неэтические как неправильные или ошибочные. Система ИИ, предназначенная для придания равного веса качеству и эффективности, даст результаты, отличные от системы, полностью ориентированной на эффективность. Кроме того, проектирование ИИ-системы с несколькими независимыми и противоречивыми целями может добавить ответственность системе.
